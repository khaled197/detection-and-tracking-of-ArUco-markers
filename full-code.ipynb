{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera access information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username =\"admin\"\n",
    "password = \"nasr1234\"\n",
    "cctv_ip_1 = \"192.168.0.151\"\n",
    "cctv_ip_2 = \"192.168.0.152\"\n",
    "\n",
    "cctv_1_url = f'rtsp://{username}:{password}@{cctv_ip_1}/cam/realmonitor?channel=1&subtype=00&authbasic=YWRtaW46YWRtaW4='\n",
    "cctv_2_url = f'rtsp://{username}:{password}@{cctv_ip_2}/cam/realmonitor?channel=1&subtype=00&authbasic=YWRtaW46YWRtaW4='\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import stitching\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions \n",
    "- Function 1 & 2: to plot images\n",
    "- Function 3: to resize frames\n",
    "- Function 3: to save videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, figsize_in_inches=(5,5)):\n",
    "    fig, ax = plt.subplots(figsize=figsize_in_inches)\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(imgs, figsize_in_inches=(5,5)):\n",
    "    fig, axs = plt.subplots(1, len(imgs), figsize=figsize_in_inches)\n",
    "    for col, img in enumerate(imgs):\n",
    "        axs[col].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show(block=False)\n",
    "\n",
    "def resize_image(img, percent = 50):\n",
    "    scale_percent = percent\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized_img\n",
    "\n",
    "def write_video(cap):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter('out_camera.avi', fourcc, fps, (width, height))\n",
    "    return out\n",
    "\n",
    "# Sometimes, and due to codec related issues, cv2.videoWriter metod does not work. In that case, use imageio\n",
    "def write_video_imageio(cap):\n",
    "    writer = imageio.get_writer('tracking.avi', fps=cap.get(cv2.CAP_PROP_FPS))\n",
    "    return writer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caibrate the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(board_size, square_size, calib_file_dest_path, calibration_images):\n",
    "    \n",
    "    #used for calibration\n",
    "    board_size = (11, 7)\n",
    "    square_size = 70  # Length of one side of a square in mm\n",
    "\n",
    "    calib_file_dest_path = \"./Calibration_data/calibration_data_Mob_MRS.npz\"\n",
    "    obj_points = []  # 3D points in real world space\n",
    "    img_points = []  # 2D points in image plane\n",
    "\n",
    "    objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)\n",
    "    objp[:, :2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1, 2) * square_size\n",
    "\n",
    "    calibration_images = glob.glob('./Calibration_data/MOB_Images/*.png')\n",
    "\n",
    "    for frame in calibration_images:\n",
    "        img = cv2.imread(frame)\n",
    "        resized_img = resize_image(img, percent = 30)\n",
    "        gray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ret, corners = cv2.findChessboardCorners(gray, board_size, None)\n",
    "\n",
    "        if ret:\n",
    "            obj_points.append(objp)\n",
    "\n",
    "            corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))\n",
    "            img_points.append(corners2)\n",
    "\n",
    "            img = cv2.drawChessboardCorners(resized_img, board_size, corners2, ret)\n",
    "            cv2.imshow('Checkerboard', resized_img)\n",
    "            cv2.waitKey(10)\n",
    "    \n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, gray.shape[::-1], None, None)\n",
    "\n",
    "    # Save the calibration data as an npz file\n",
    "    np.savez(calib_file_dest_path, mtx=mtx, dist=dist)\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_file_path = './Calibration_data/calibration_data_cctv_1.npz'\n",
    "# calibration_file_path = './Calibration_data/calibration_data_webcam_1.npz'\n",
    "\n",
    "with np.load(calibration_file_path) as data:\n",
    "\n",
    "    cam_mtx, dist_coeff = data['mtx'], data['dist']\n",
    "\n",
    "#For mobile camera, we can assume there is no distortion\n",
    "zero_dist_coeff = np.array([[0.0, 0.0, 0.0, 0.0, 0.0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define three functions:\n",
    " - Function 1: undistort the frame\n",
    " - Function 2: create the auco marker detector\n",
    " - Function 3: stitch the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_frame(frame, mtx, dist):\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # mtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 0, (w, h))\n",
    "\n",
    "    undistorted_frame = cv2.undistort(frame, mtx, dist)\n",
    "\n",
    "    #For most cases, this step doesn't produce better results\n",
    "    #x, y, w, h = roi\n",
    "    #undistorted_frame = undistorted_frame[y:y+h, x:x+w]\n",
    "\n",
    "    return undistorted_frame\n",
    "\n",
    "def create_detector(dictionary = cv2.aruco.DICT_5X5_250):\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(dictionary)\n",
    "    params = cv2.aruco.DetectorParameters()\n",
    "    return cv2.aruco.ArucoDetector(aruco_dict, params)\n",
    "\n",
    "#used to stitch unsaved frames, since the stitcher class only accepts file paths\n",
    "def stitch(stitcher, frames):\n",
    "\n",
    "    temp_files = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        temp_file = f\"temp_{i}.jpg\"\n",
    "        cv2.imwrite(temp_file, frame)\n",
    "        temp_files.append(temp_file)\n",
    "\n",
    "    stitched_image = stitcher.stitch([temp_files[0], temp_files[1]])\n",
    "\n",
    "    for temp_file in temp_files:\n",
    "        os.remove(temp_file)\n",
    "    \n",
    "    return stitched_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Perform projective transformortaion if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "src_points = []\n",
    "dst_points = []\n",
    "\n",
    "def mouse_click(event, x, y, flags, param):\n",
    "    # Handle mouse click events\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if len(src_points) < 4:\n",
    "            # Add the clicked point to the source points list\n",
    "            src_points.append((x, y))\n",
    "            cv2.circle(input_image, (x, y), 5, (0, 0, 255), -1)\n",
    "            cv2.imshow(\"Input Image\", input_image)\n",
    "\n",
    "\n",
    "# Load the input image\n",
    "input_image = cv2.imread('./hellos_KF_2.png')\n",
    "# input_image = resize_image(image, percent = 30)\n",
    "height, width = input_image.shape[:2]\n",
    "\n",
    "# Create windows and set mouse click callback\n",
    "cv2.namedWindow(\"Input Image\", cv2.WINDOW_NORMAL)\n",
    "# cv2.namedWindow(\"Transformed Image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "cv2.setMouseCallback(\"Input Image\", mouse_click)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"Input Image\", input_image)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"r\"):\n",
    "        src_points = []\n",
    "        dst_points = []\n",
    "        # input_image = resize_image(image, percent = 30)\n",
    "        cv2.destroyWindow(\"Input Image\")\n",
    "        input_image = cv2.imread('./hellos_KF_2.png')\n",
    "        cv2.namedWindow(\"Input Image\", cv2.WINDOW_NORMAL)\n",
    "        cv2.setMouseCallback(\"Input Image\", mouse_click)\n",
    "        cv2.imshow(\"Input Image\", input_image)\n",
    "        if cv2.getWindowProperty(\"Transformed Image\", cv2.WND_PROP_VISIBLE) >= 1:\n",
    "            cv2.destroyWindow(\"Transformed Image\")\n",
    "\n",
    "    elif key == ord(\"c\"):\n",
    "        width, height = input_image.shape[:2]\n",
    "        dst_points.append((0, 0))\n",
    "        dst_points.append((width, 0))\n",
    "        dst_points.append((width, height))\n",
    "        dst_points.append((0, height))\n",
    "        if len(src_points) == 4:\n",
    "            # Calculate and display the perspective transformation\n",
    "            M = cv2.getPerspectiveTransform(np.float32(src_points), np.float32(dst_points))\n",
    "            transformed_image = cv2.warpPerspective(input_image, M, (width, height))\n",
    "            cv2.imshow(\"Transformed Image\", transformed_image)\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        # cv2.imwrite(\"transformed_image_right.jpg\", transformed_image)\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stitcher\n",
    "settings = {\"detector\": \"sift\", \"confidence_threshold\": 0.1}\n",
    "stitcher = stitching.Stitcher(**settings)\n",
    "\n",
    "# read the images for stitching\n",
    "city_imgs = []  \n",
    "for path in Path('./Mobile_Dataset/New folder/first').glob(\"*\"):\n",
    "    city_imgs.append(str(path))\n",
    "\n",
    "# stitch the images\n",
    "stitched_image = stitcher.stitch(stitcher, city_imgs)\n",
    "\n",
    "## These are optional steps to detect keypoints and matches between images and draw them if needed\n",
    "\n",
    "img_handler = stitching.image_handler.ImageHandler(medium_megapix=0.6, low_megapix=0.1, final_megapix=-1)\n",
    "img_handler.set_img_names(city_imgs)\n",
    "\n",
    "# resize images\n",
    "medium_imgs = list(img_handler.resize_to_medium_resolution())\n",
    "low_imgs = list(img_handler.resize_to_low_resolution(medium_imgs))\n",
    "final_imgs = list(img_handler.resize_to_final_resolution())\n",
    "# plot_images(list(final_imgs), (20,20))\n",
    "\n",
    "#draw detected keypoints\n",
    "finder = stitching.feature_detector.FeatureDetector()\n",
    "features = [finder.detect_features(img) for img in medium_imgs]\n",
    "keypoints_center_img_1 = finder.draw_keypoints(medium_imgs[0], features[0])\n",
    "keypoints_center_img_2 = finder.draw_keypoints(medium_imgs[1], features[1])\n",
    "keypoints_center_img_3 = finder.draw_keypoints(medium_imgs[1], features[1])\n",
    "# plot_images([keypoints_center_img_1,keypoints_center_img_2, keypoints_center_img_3])\n",
    "\n",
    "#plot matched features\n",
    "matcher = stitching.feature_matcher.FeatureMatcher()\n",
    "matches = matcher.match_features(features)\n",
    "print(matches)\n",
    "print(matcher.get_confidence_matrix(matches))\n",
    "\n",
    "all_relevant_matches = matcher.draw_matches_matrix(final_imgs, features, matches, conf_thresh=0.01, \n",
    "                                                    inliers=True, matchColor=(0, 255, 0))\n",
    "print(all_relevant_matches)\n",
    "for idx1, idx2, img in all_relevant_matches:\n",
    "    print(idx1, idx2)\n",
    "    print(f\"Matches Image {idx1+1} to Image {idx2+1}\")\n",
    "    plot_image(img, (200,300))\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect the aruco markers and calculate their actual and relative coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArUco marker ID from which positions orientations of\n",
    "#    all other aruco markers are measured.\n",
    "world_id = 10\n",
    "\n",
    "# Size of the ArUco marker in mm\n",
    "marker_size = 200\n",
    "\n",
    "\"\"\"\n",
    "The correction factor is used to correct the position\n",
    "    of the ArUco marker in the distortion axis of the camera.\n",
    "It's estimated by trial and error\n",
    "\"\"\"\n",
    "correction_factor = 1.2/1.35\n",
    "\n",
    "# initialize the ArUco detector\n",
    "detector = create_detector()\n",
    "\n",
    "#capture the frame\n",
    "# cap = cv2.VideoCapture('help.avi')\n",
    "#or\n",
    "#read the image\n",
    "undistorted_frame = cv2.imread(\"./Webcam1275.jpg\")   \n",
    "\n",
    "# writer = write_video(cap)\n",
    "# or\n",
    "# writer = write_video_imageio(cap)\n",
    "\n",
    "while True:\n",
    "\n",
    "    # ret, undistorted_frame = cap.read()\n",
    "    # if not ret:\n",
    "    #     break\n",
    "\n",
    "    # resize the image in case it's too large\n",
    "    # resized_img = resize_image(frame, percent = 75) \n",
    "\n",
    "    # This step does not make a difference in the results\n",
    "    # original_gray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # undistort the frame\n",
    "    # undistorted_frame = undistort_frame(resized_img, cam_mtx, dist_coeff)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # In case the image is not bright enough for ArUco detection, we can use a kernel to increase the brightness\n",
    "    # kernel = np.array([[-1,-1,-1], [-1,11,-1], [-1,-1,-1]])    \n",
    "    # undistorted_frame = cv2.filter2D(resized_img, -1, kernel)\n",
    "\n",
    "    # detect the ArUco markers\n",
    "    corners, ids, _ = detector.detectMarkers(undistorted_frame)\n",
    "\n",
    "    if ids is None:\n",
    "    #   cv2.rectangle(resized_img, (0, 0), (resized_img.shape[1], resized_img.shape[0]), (0, 0, 255), thickness=2)\n",
    "    #   cv2.putText(resized_img,\"No aruco markers detected\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    #   cv2.imshow('empty_frame', undistorted_frame)\n",
    "        print(\"No aruco markers detected\")\n",
    "        continue\n",
    " \n",
    "\n",
    "    if cv2.getWindowProperty('empty_frame', cv2.WND_PROP_VISIBLE) == 1:\n",
    "        cv2.destroyWindow('empty_frame')\n",
    "\n",
    "    # draw boxes around the detected ArUco markers\n",
    "    cv2.aruco.drawDetectedMarkers(undistorted_frame, corners, ids)\n",
    "\n",
    "    # estimate the pose of the ArUco markers\n",
    "    rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, marker_size, cam_mtx, zero_dist_coeff)\n",
    "\n",
    "    for i, id in enumerate(ids.flatten()):\n",
    "        if id == world_id:\n",
    "            world_pos_vec = tvecs[i].squeeze()\n",
    "            world_rot_vec = rvecs[i].squeeze()\n",
    "\n",
    "            # For debugging purposes\n",
    "            print(\"################################################## \\n\")\n",
    "            print(f\"position vector of world aruco marker with {id} = {world_pos_vec[:2]/1000} \\n\")\n",
    "            print(f\"rotation matrix of world aruco marker with {id} = {world_rot_vec} \\n\")\n",
    "\n",
    "            # draw the coordinate axes of the world ArUco marker\n",
    "            cv2.drawFrameAxes(undistorted_frame, cam_mtx, zero_dist_coeff, world_rot_vec, world_pos_vec, marker_size) \n",
    "            cv2.putText(undistorted_frame, \"World\", (int(corners[i][0][0][0] + 10), int(corners[i][0][0][1] + 45)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)                   \n",
    "        else:\n",
    "            world_idx = np.argmax(ids == world_id)\n",
    "\n",
    "            # The translation and rotation vectors of the world ArUco marker\n",
    "            world_pos_vec = tvecs[world_idx].squeeze()\n",
    "            world_rot_vec = rvecs[world_idx].squeeze()\n",
    "\n",
    "\n",
    "            # The translation and rotation vectors of the other ArUco marker\n",
    "            aruco_pos_vec = tvecs[i].squeeze()\n",
    "            aruco_rot_vec = rvecs[i].squeeze()\n",
    "\n",
    "            # convert the rotation vector to a rotation matrix\n",
    "            world_rot_mtx, _ = cv2.Rodrigues(world_rot_vec)\n",
    "            aruco_rot_mtx, _ = cv2.Rodrigues(aruco_rot_vec)\n",
    "            \n",
    "            # calculate the translation vector and rotation vector of the other ArUco marker relative to the world ArUco marker\n",
    "            aruco_rel_pos_vec = world_rot_mtx.T @ (aruco_pos_vec - world_pos_vec)\n",
    "            aruco_rel_rot_mtx = aruco_rot_mtx @ world_rot_mtx.T\n",
    "\n",
    "            # extract the yaw angle of the other ArUco marker relative to the world ArUco marker\n",
    "            yaw_angle_rad = np.arctan2(aruco_rel_rot_mtx[0, 1], aruco_rel_rot_mtx[0, 0])\n",
    "\n",
    "            # convert the yaw angle from radians to degrees\n",
    "            yaw_angle_deg = np.rad2deg(yaw_angle_rad)\n",
    "\n",
    "            # correct the position of the other ArUco marker in the distortion axis of the cctv camera\n",
    "            #    which in this case is the y-axis\n",
    "            aruco_rel_pos_vec[1] *= correction_factor\n",
    "\n",
    "            # For debugging purposes\n",
    "            print(\"################################################## \\n\")\n",
    "            print(f\"Relative position vector for marker with id {id} \", aruco_rel_pos_vec[:2]/1000)\n",
    "            print(f\"The yaw angle for marker with id {id}:\", yaw_angle_deg)\n",
    "\n",
    "            # draw the coordinate axes of the ArUco marker\n",
    "            cv2.drawFrameAxes(undistorted_frame, cam_mtx, zero_dist_coeff, aruco_rot_vec, aruco_pos_vec, marker_size) \n",
    "\n",
    "\n",
    "            # draw the information of each ArUco marker\n",
    "            info_1 = f\"Position: {round(aruco_rel_pos_vec[0]/1000, 3)}, {round(aruco_rel_pos_vec[1]/1000, 3)} m\"\n",
    "            info_2 = \"Orientation :\" + str(round(yaw_angle_deg, 3)) + \" deg\"\n",
    "            cv2.putText(undistorted_frame, info_1, (int(corners[i][0][0][0] - 120), int(corners[i][0][0][1] - 40)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "            cv2.putText(undistorted_frame, info_2, (int(corners[i][0][0][0] - 120), int(corners[i][0][0][1]) -20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        \n",
    "    if undistorted_frame is not None:\n",
    "        cv2.imshow('frame', undistorted_frame)\n",
    "        cv2.imwrite(\"./R_555_webcam\", undistorted_frame)\n",
    "        # to save a video \n",
    "        # writer.write(undistorted_frame)\n",
    "        # or\n",
    "        # writer.append_data(cv2.cvtColor(undistorted_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # to save a frame\n",
    "        # cv2.imwrite(f'./estimate_pose/{idx}.jpg', undistorted_frame)\n",
    "\n",
    "# cap.release()\n",
    "# writer.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
